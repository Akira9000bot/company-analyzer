# 2026-02-23

## Cache Fixes Discussion & Implementation

**Context:** Dan asked about "cache fixes" - referred to JSON escaping and caching bugs fixed in the company-analyzer trace/logging infrastructure.

**Previously committed (de527ef):**
- lib/cache.sh: Use printf | jq -Rs for proper response handling
- lib/api-client.sh: Add missing function exports (extract_usage, etc.)
- lib/trace.sh: Fix SCRIPT_DIR dependency, use BASH_SOURCE
- run-framework.sh: Fix unbound variable error with init_trace, make cache_get failures non-fatal
- Fixed bugs discovered during COIN analysis testing

**Additional uncommitted changes committed and pushed (09d52d1):**
- cache.sh: Use temp files + jq --rawfile for safe special char handling
- cache.sh: Move cache from /tmp/ to skill-local .cache/ directory
- cache.sh: Add metadata JSON validation with fallback to empty object
- fetch_data.sh: Additional caching improvements
- cost-tracker.sh: Updates
- analyze-parallel.sh: Minor fixes

**Repository:** https://github.com/Akira9000bot/company-analyzer

---

## Synthesis Phase Removed - 40% Cost Reduction

**Decision:** Removed the 09-synthesis framework that generated BUY/HOLD/SELL verdicts.

**Rationale:** 
- Synthesis was ~40% of analysis cost (~$0.01 of ~$0.045 total)
- User can read 8 framework outputs directly without AI-generated summary
- Faster execution (4s vs 6s)

**Changes made (commit 7c94d00):**
- analyze-parallel.sh: Removed synthesis API call, now displays all 8 framework summaries
- SKILL.md: Updated documentation to reflect new behavior
- Telegram delivery: Now sends framework excerpts instead of verdict

**New costs:**
- Per-analysis: ~$0.03 (down from ~$0.045)
- 8 frameworks only, no synthesis

**Trade-offs:**
- ❌ No consolidated BUY/HOLD/SELL verdict
- ❌ No narrative flip detection logic
- ✅ 40% cost savings
- ✅ ~33% faster execution
- ✅ User gets raw framework outputs to interpret

---

## Cost Architecture Analysis Performed

Analyzed company-analyzer cost breakdown:

**Pricing (Moonshot/Kimi k2.5):**
- Input: $0.60 per 1M tokens
- Output: $3.00 per 1M tokens (5x more expensive)

**Biggest cost drivers identified:**
1. **Synthesis (09-synthesis)** - ~40% of cost - NOW REMOVED
2. **High-output frameworks:** 05-sentiment, 06-growth, 07-business (~1,900-2,100 tokens each)
3. **Output explosion:** Output accounts for ~99% of total cost

**Per-framework costs (before removal):**
| Framework | Avg Cost |
|-----------|----------|
| 05-sentiment | $0.0063 |
| 06-growth | $0.0059 |
| 07-business | ~$0.0056 |
| 04-strategic-moat | $0.0052 |
| 08-risk | $0.0047 |
| 02-metrics | $0.0043 |
| 01-phase | $0.0031 |
| 09-synthesis | ~$0.009+ |

**Optimization opportunities identified (synthesis already removed):**
- [x] Add strict output limits to high-token frameworks (05, 06, 07) - IMPLEMENTED but BUGGY
- [ ] Cache synthesis results (N/A now) - REMOVED
- [ ] Compress framework outputs - 10-15% savings

---

## CRITICAL BUG: Kimi k2.5 Reasoning Mode Consumes All Tokens

**Issue discovered during UBER analysis (2026-02-23 04:44-04:48)**

**Problem:** Kimi k2.5 has "reasoning mode" enabled by default. When `max_tokens` is set, the model uses tokens for internal reasoning/thinking BEFORE generating output content. When reasoning exceeds the limit, `content` is empty but API still charges.

**Impact:**
- First UBER analysis (02-23 04:38): $0.01888 charged, 7/8 frameworks returned empty files
- Second UBER analysis (02-23 04:44): $0.02836 charged, 5/8 frameworks returned empty files
- Total wasted: ~$0.047 for no usable output

**Evidence:**
```json
"content": "",
"reasoning_content": "5853 chars of reasoning...",
"finish_reason": "length",
"usage": {"completion_tokens": 1200}
```

**Root cause:** 1200 tokens spent on reasoning → 0 tokens for content → empty output file

**Attempted fix:** Increased max_tokens from 600-900 to 1200-1500
**Result:** Still failing - reasoning consumes all available tokens

**Next steps:**
1. Switch to `google/gemini-3-flash` (no reasoning overhead, 3x cheaper)
2. OR massively increase limits to 2000+ tokens (expensive)
3. OR investigate Moonshot API parameter to disable reasoning

**Lesson:** Token limits on reasoning models must account for reasoning overhead, not just output content.

